---
title: "p8105_hw5_kp3127"
output: html_document
date: "2025-11-05"
---
```{r setup, echo=FALSE}
library(tidyverse)
library(rvest)
library(patchwork)
library(purrr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
set.seed(1)
```

# Problem 1
Write a function to check whether there are duplicate birthdays in the group
```{r}
bd_check <- function(n, days = 365) {
  bdays <- sample.int(days, size = n, replace = TRUE)
  any(duplicated(bdays))
}
```

Compute the probability for each group size between 2 and 50, running 10000 times:
```{r}
sim_df=
  tibble(gp_size=2:50) %>% 
  mutate(prob=map_dbl(.x=gp_size,~mean(replicate(10000,bd_check(.x)))))
```

```{r}
sim_df |>
  ggplot(aes(x = gp_size, y = prob)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.5, linetype = 2) +
  labs(
    title = "Birthday problem: Pr(≥2 share a birthday) vs group size",
    x = "Group size (n)",
    y = "Estimated probability (10,000 sims per n)"
  ) +
  theme_minimal()
```

The curve rises monotonically with group size. It crosses 0.50 when n = 23.  
This result matches the classic birthday paradox: with just 23 people, there’s over a 50% chance at least two share a birthday.

# Problem 2
```{r}
library(broom)
```

For a given mu, create a function to get the results of mu_hat and p-value
```{r}
set.seed(1)
n      <- 30
sigma  <- 5
mus    <- 0:6
n_sim  <- 5000
alpha  <- 0.05

sim_ttest = function(mu) {
  map_dfr(1:n_sim, function(i) {
    x = rnorm(n, mean = mu, sd = sigma)
    ttest = tidy(t.test(x, mu = 0))
    tibble(
      mu = mu,
      mu_hat = ttest$estimate,   
      p = ttest$p.value,    
      reject = p < alpha
    )
  })
}
```

Repeat the function for different mu[0:6] 
```{r}
sim_res=map_dfr(mus, sim_ttest)
```

Calculate the power
```{r}
power_df=sim_res %>% 
  group_by(mu) %>% 
  summarize(power=mean(reject))
```

Describe the association between effect size and power.
```{r}
power_df %>% 
  ggplot(aes(mu, power)) +
  geom_point() + geom_line() +
  scale_x_continuous(breaks = mus, limits = c(0, 6)) +
  labs(title = "Power of one-sample t-test (n = 30, σ = 5, α = 0.05)",
       x = "True mean (μ)", y = "Estimated power (5,000 sims)") +
  theme_minimal()
```

Calculate the mu_hat
```{r}
mu_df=sim_res %>% 
  group_by(mu) %>% 
  summarize(
    mu_hat_all=mean(mu_hat),
    mu_hat_rej=mean(mu_hat[reject==TRUE])
  )
```

Create a plot to compare true mu and mu_hat
```{r}
mu_df |>
  pivot_longer(-mu, names_to = "which", values_to = "avg_mu_hat") |>
  ggplot(aes(mu, avg_mu_hat, color = which)) +
  geom_point() + geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  scale_x_continuous(breaks = 0:6, limits = c(0, 6)) +
  labs(title = "Average sample mean vs true μ",
       x = "True μ", y = "Average  μ̂ (all vs only rejected)") +
  theme_minimal(base_size = 12)
```
All samples (mu_hat_all): essentially unbiased.   
Significant-only (mu_hat_rej): inflated when power < 1, then converges to μ as power → 1.  
This is selection on statistical significance (the “winner’s curse”). When power isn’t 1, only datasets where random error pushes mu upward tend to pass p<0.05, so the conditional average among rejections is biased high. As the true effect grows (power → 1), nearly every sample rejects and the selection bias vanishes.

# Problem 3
```{r}
homi = read_csv("./homicide-data.csv") %>%  
  janitor::clean_names()
```

Describe the raw data
```{r}
n_rows   = nrow(homi)
n_cols   = ncol(homi)
city_n   = homi %>%  distinct(city, state) %>%  nrow()
year_min = homi %>%  summarise(y = min(year(ymd(reported_date)), na.rm = TRUE)) %>%  pull(y)
year_max = homi %>%  summarise(y = max(year(ymd(reported_date)), na.rm = TRUE)) %>%  pull(y)
```

This raw dataset contains **one row per homicide case**, with `r n_rows` observations and `r n_cols` variables, spanning `r city_n` distinct city–state combinations from **`r year_min`** to **`r year_max`**. Core fields include the report date, victim demographics (age, sex, race), location (city, state, latitude/longitude), and the case disposition.

Create the new variable
```{r}
homi=homi %>% 
  mutate(
    city_state = paste(city, state, sep = ", "),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  )

city_sum <- homi |>
  group_by(city_state) |>
  summarise(
    total    = n(),
    unsolved = sum(unsolved, na.rm = TRUE),
    .groups  = "drop"
  ) %>% 
  arrange(desc(total))

Baltimore_homi =city_sum %>% 
  filter(city_state == "Baltimore, MD")
head(Baltimore_homi)
```
Apply r`prop.test` function to Baltimore: 
```{r}
pt_balt <- prop.test(
  x = Baltimore_homi$unsolved,
  n = Baltimore_homi$total
)

balt_est <- broom::tidy(pt_balt) |>
  select(estimate, conf.low, conf.high)

balt_est
```

Apply `prop.test` function to all: 
```{r}
pb_city = city_sum %>% 
  transmute(
    city_state,
    x = as.integer(unsolved),
    n = as.integer(total)
  ) %>% 
  mutate(
    test = map2(x, n, ~ prop.test(.x, .y)),   
    tidy = map(test, broom::tidy)
  ) %>% 
  select(-test) %>% 
  unnest(tidy) %>% 
  select(city_state, x, n, estimate, conf.low, conf.high) %>% 
  arrange(desc(estimate))

pb_city
```

Now we make a plot:
```{r}
pb_city %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(city_state, estimate)) +
  geom_point(size = 1.8) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Proportion of unsolved homicides by city",
    x = NULL,
    y = "Proportion unsolved (95% CI)"
  ) +
  theme_minimal()

```

